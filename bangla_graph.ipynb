{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "lines = \"\"\"চলি চলো চলিস চলে চলেন\n",
        "চলছি চলছ চলছিস চলছে চলছেন\n",
        "চলেছই চলেছও চলেছিস চলেছে চলেছেন\n",
        "চলো চলিস চলুক চলুন\n",
        "চললাম চললে চললি চললে চললেন\n",
        "চলছিলাম চলছিলে চলছিলি চলছিল চলছিলেন\n",
        "চলেছিলাম চলেছিলে চলেছিলি চলেছিল চলেছিলেন\n",
        "চলতাম চলতে চলতি চলত চলতেন\n",
        "চলব চলবে চলবি চলবেন\"\"\"\n",
        "\n",
        "list_of_lists = [line.split() for line in lines.split('\\n')]\n",
        "\n",
        "print(list_of_lists)\n"
      ],
      "metadata": {
        "id": "X92oZcIuO1uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSiFAOjCw5p-"
      },
      "outputs": [],
      "source": [
        "original_list = [['করি', 'করো', 'করিস', 'করে', 'করেন'],\n",
        "['করছি', 'করছ', 'করছিস', 'করছে', 'করছেন'],\n",
        "['করেছি', 'করেছ', 'করেছিস', 'করেছে', 'করেছেন'],\n",
        "['করো', 'করিস', 'করুক', 'করুন'],\n",
        "['করলাম', 'করলে', 'করলি', 'করল', 'করলেন'],\n",
        "['করছিলাম', 'করছিলে', 'করছিলি', 'করছিল', 'করছিলেন'],\n",
        "['করেছিলাম', 'করেছিলে', 'করেছিলি', 'করেছিল', 'করেছিলেন'],\n",
        "['করতাম', 'করতে', 'করতি', 'করত', 'করতেন'],\n",
        "['করব', 'করবে', 'করবি', 'করবেন']]\n",
        "\n",
        "pairs = []\n",
        "for l in range(len(original_list[0])):\n",
        "  for r in range(l+1, len(original_list[0])):\n",
        "    pairs.extend([[lst[0], lst[1]] for lst in original_list])\n",
        "\n",
        "print(pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzz6jP6BDOz4"
      },
      "outputs": [],
      "source": [
        "puzzle = '''\n",
        "পূর্বে আগে\n",
        "পূর্বেই আগেই\n",
        "অদ্য আজ\n",
        "সহিত সাথে\n",
        "দিয়া দিয়ে\n",
        "অদ্য\tআজ\n",
        "অদ্যাপি\tআজও\n",
        "তথাপি\tতবুও\n",
        "নতুবা\tনইলে\n",
        "নতুবা নাহলে\n",
        "প্রায়শ প্রায়ই\n",
        "যদ্যপি\tযদিও\n",
        "কুত্রাপি\tকোথাও\n",
        "কিঞ্চিৎ\tকিছু\n",
        "কিঞ্চিৎ কিছুটা\n",
        "'''\n",
        "\n",
        "puzzle_list = puzzle.split('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7cEhJW6Dah4"
      },
      "outputs": [],
      "source": [
        "puzzle_list = [i.strip() for i in puzzle_list]\n",
        "puzzle_list = [i.rstrip() for i in puzzle_list]\n",
        "puzzle_list = list(filter(None, puzzle_list))\n",
        "print(puzzle_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pvcKI6V4lKw"
      },
      "outputs": [],
      "source": [
        "res = []\n",
        "for p in puzzle_list:\n",
        "  words = p.split()\n",
        "  r_p = ' '.join(words)\n",
        "\n",
        "  if r_p not in res:\n",
        "    res.append(r_p)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFrDIPQAdYAi"
      },
      "source": [
        "#Total Count of Bangla NLP Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv0FDoIzdhsa"
      },
      "source": [
        "#Filter Mikolov Translate File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-MfYiZYtdXH0"
      },
      "outputs": [],
      "source": [
        "#See if any word has not been\n",
        "import re\n",
        "\n",
        "# Set the filename for the input and output files\n",
        "input_filename = 'Mikolov.txt'\n",
        "output_filename = 'Mikolov-Filtered.txt'\n",
        "\n",
        "# Set the regular expression pattern to match English words\n",
        "english_word_pattern = re.compile(r'\\b[a-zA-Z]+\\b')\n",
        "\n",
        "# Open the input and output files\n",
        "with open(input_filename, 'r') as input_file, open(output_filename, 'w') as output_file:\n",
        "    # Loop through each line in the input file\n",
        "    for line in input_file:\n",
        "        # Check if the line contains an English word\n",
        "        if not re.search(english_word_pattern, line):\n",
        "            # Write the line to the output file if it doesn't contain an English word\n",
        "            output_file.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLR2wL3geLJU"
      },
      "outputs": [],
      "source": [
        "#Remove phrases or more than 4 words\n",
        "with open('Mikolov-Filtered.txt', 'r') as infile, open('Mikolov-Translate-Filtered_2.txt', 'w') as outfile:\n",
        "    for line in infile:\n",
        "        if len(line.split()) == 4:\n",
        "            outfile.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HLV2XzNezpT"
      },
      "outputs": [],
      "source": [
        "#Remove Duplicates\n",
        "with open('Mikolov-Translate-Filtered_2.txt', 'r') as input_file, open('Mikolov-Translate-Filtered_3.txt', 'w') as output_file:\n",
        "    for line in input_file:\n",
        "        words = line.strip().split()\n",
        "        if len(set(words)) == len(words):\n",
        "            output_file.write(' '.join(words))\n",
        "            output_file.write('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Word Embedding"
      ],
      "metadata": {
        "id": "lSCXxj-x03a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "U3xkVf5s1JP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def get_word_idx(sent: str, word: str):\n",
        "    return sent.split(\" \").index(word)\n",
        "\n",
        "def get_hidden_states(encoded, token_ids_word, model, layers):\n",
        "     \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
        "        Select only those subword token outputs that belong to our word of interest\n",
        "        and average them.\"\"\"\n",
        "     with torch.no_grad():\n",
        "         output = model(**encoded)\n",
        "\n",
        "     # Get all hidden states\n",
        "     states = output.hidden_states\n",
        "     # Stack and sum all requested layers\n",
        "     output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
        "     # Only select the tokens that constitute the requested word\n",
        "     word_tokens_output = output[token_ids_word]\n",
        "\n",
        "     return word_tokens_output.mean(dim=0)\n",
        "\n",
        "\n",
        "def get_word_vector(sent, idx, tokenizer, model, layers):\n",
        "     \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
        "        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
        "     encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
        "     # get all token idxs that belong to the word of interest\n",
        "     token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
        "\n",
        "     return get_hidden_states(encoded, token_ids_word, model, layers)\n",
        "\n",
        "\n",
        "def get_embedding(layers=None, str_=\"\"):\n",
        "     # Use last four layers by default\n",
        "     layers = [-4, -3, -2, -1] if layers is None else layers\n",
        "     tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "     model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
        "\n",
        "     embeddings = []\n",
        "\n",
        "     for idx, w in enumerate(str_.split()):\n",
        "        embeddings.append(get_word_vector(str_, idx, tokenizer, model, layers))\n",
        "\n",
        "     return torch.stack(embeddings).to(device)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "     embd = get_embedding(str_ = \"I like river side\")\n",
        "     print(embd.shape)"
      ],
      "metadata": {
        "id": "QCK4eNpq06EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# create two matrices of vectors\n",
        "A = torch.randn(10, 5)  # 10 vectors of length 5\n",
        "B = torch.randn(8, 5)   # 8 vectors of length 5\n",
        "\n",
        "# compute the cosine similarity between the two matrices\n",
        "similarity_matrix = cosine_similarity(A, B)\n",
        "\n",
        "# the similarity matrix is of shape (10, 8), where each element [i, j] is the\n",
        "# cosine similarity between vector i in A and vector j in B\n",
        "print(similarity_matrix.shape)"
      ],
      "metadata": {
        "id": "Dpn_ucno7O6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# text = \"Special counsel Robert Mueller's team, Trump campaign officials, Jared Kushner\"\n",
        "text = \"Special Counsel Robert Mueller, Trump campaign officials, former campaign manager Paul Manafort, former campaign official George Papadopoulos, President Trump, Attorney General Jeff Sessions, federal officials.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "IoCBIwpapPaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bnlp_toolkit"
      ],
      "metadata": {
        "id": "5hGXrWV7-G3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import BengaliGlove\n",
        "\n",
        "# Load the BengaliGlove embeddings\n",
        "glove = BengaliGlove()\n",
        "\n",
        "# Path to the embeddings file\n",
        "embedding_file = glove.closest_word\n",
        "\n",
        "# Create a dictionary to hold the word vectors\n",
        "word_dict = {}\n",
        "\n",
        "# Iterate over the embeddings file\n",
        "with open(embedding_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.strip().split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        word_dict[word] = vector\n",
        "\n",
        "# Print the length of the word dictionary\n",
        "print(len(word_dict))"
      ],
      "metadata": {
        "id": "lH3dFwg_990D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "_dazCtcNgBI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"setu4993/LaBSE-Bengali\")\n",
        "model = AutoModel.from_pretrained(\"setu4993/LaBSE-Bengali\")\n",
        "sentence = \"আমি ভাত খাই।\"\n",
        "input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "word_embeddings = embeddings.squeeze(0).split(1)\n",
        "word_embeddings = [we.squeeze(0) for we in word_embeddings]"
      ],
      "metadata": {
        "id": "n04u4Z0Dp1RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "89fAlNs3qvz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name = 'sentence-transformers/LaBSE'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Example Bangla sentence\n",
        "sentence = 'আমি বাংলায় কথা বলছি।'\n",
        "\n",
        "# Compute LaBSE embedding\n",
        "embedding = model.encode(sentence)\n",
        "\n",
        "print(embedding)\n"
      ],
      "metadata": {
        "id": "w57-vd1eqtP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\")\n",
        "\n",
        "sentence = 'আমি'\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    last_hidden_states = outputs[0]\n",
        "\n",
        "word_embeddings = []\n",
        "for i, token in enumerate(tokens):\n",
        "    embedding = last_hidden_states[0][i+1]\n",
        "    word_embeddings.append(embedding)\n",
        "\n",
        "print(len(word_embeddings))\n",
        "print(tokenizer.get_vocab())\n",
        "print(tokenizer.ge)"
      ],
      "metadata": {
        "id": "RqANYK4LsBli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "# Load the LaBSE model\n",
        "model_name = 'sentence-transformers/LaBSE'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Define the Mikolov-style word analogy\n",
        "a = 'লম্বা'\n",
        "b = 'চারদিকে'\n",
        "c = 'দূরত্ব'\n",
        "\n",
        "# Tokenize the words\n",
        "a_tokens = model.tokenize(a)\n",
        "b_tokens = model.tokenize(b)\n",
        "c_tokens = model.tokenize(c)\n",
        "\n",
        "# Encode the words\n",
        "a_embedding = model.encode(a)\n",
        "b_embedding = model.encode(b)\n",
        "c_embedding = model.encode(c)\n",
        "\n",
        "# Compute the vector difference\n",
        "diff_vector = b_embedding - a_embedding + c_embedding\n",
        "\n",
        "# Find the closest word to the difference vector\n",
        "words = ['লম্বা', 'চারদিকে', 'দূরত্ব']\n",
        "word_embeddings = model.encode(words)\n",
        "similarity = util.pytorch_cos_sim(word_embeddings, diff_vector).cpu().detach().numpy().flatten()\n",
        "most_similar_word = words[np.argmax(similarity)]\n",
        "\n",
        "print(f'{a} is to {b} as {c} is to {most_similar_word}')"
      ],
      "metadata": {
        "id": "PoinX6P7uaEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install laserembeddings"
      ],
      "metadata": {
        "id": "PgiZwC3qvf5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m laserembeddings download-models"
      ],
      "metadata": {
        "id": "3OG5qThrv6Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from laserembeddings import Laser\n",
        "\n",
        "# Load the pre-trained Laser model\n",
        "laser = Laser()\n",
        "\n",
        "# Define the Mikolov-style word analogy\n",
        "a = 'লম্বা'\n",
        "b = 'চারদিকে'\n",
        "c = 'দূরত্ব'\n",
        "\n",
        "# Tokenize the words\n",
        "a_tokens = a.split()\n",
        "b_tokens = b.split()\n",
        "c_tokens = c.split()\n",
        "\n",
        "# Encode the words\n",
        "a_embedding = laser.embed_sentences([a_tokens], lang='bn')[0]\n",
        "b_embedding = laser.embed_sentences([b_tokens], lang='bn')[0]\n",
        "c_embedding = laser.embed_sentences([c_tokens], lang='bn')[0]\n",
        "\n",
        "# Compute the vector difference\n",
        "diff_vector = b_embedding - a_embedding + c_embedding\n",
        "\n",
        "# Find the closest word to the difference vector\n",
        "words = ['লম্বা', 'চারদিকে', 'দূরত্ব']\n",
        "word_embeddings = laser.embed_sentences([words], lang='bn')[0]\n",
        "similarity = np.dot(word_embeddings, diff_vector) / (np.linalg.norm(word_embeddings) * np.linalg.norm(diff_vector))\n",
        "most_similar_word = words[np.argmax(similarity)]\n",
        "\n",
        "print(f'{a} is to {b} as {c} is to {most_similar_word}')\n"
      ],
      "metadata": {
        "id": "cNLVK8BAyEvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the BanglaBERT model and tokenizer\n",
        "model_name = 'csebuetnlp/banglabert'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Define the Mikolov-style word analogy\n",
        "a = 'লম্বা'\n",
        "b = 'চারদিকে'\n",
        "c = 'দূরত্ব'\n",
        "\n",
        "# Tokenize the words\n",
        "a_tokens = tokenizer.tokenize(a)\n",
        "b_tokens = tokenizer.tokenize(b)\n",
        "c_tokens = tokenizer.tokenize(c)\n",
        "\n",
        "# Convert the tokens to IDs\n",
        "a_ids = tokenizer.convert_tokens_to_ids(a_tokens)\n",
        "b_ids = tokenizer.convert_tokens_to_ids(b_tokens)\n",
        "c_ids = tokenizer.convert_tokens_to_ids(c_tokens)\n",
        "\n",
        "# Convert the IDs to PyTorch tensors\n",
        "a_tensor = torch.tensor(a_ids).unsqueeze(0)\n",
        "b_tensor = torch.tensor(b_ids).unsqueeze(0)\n",
        "c_tensor = torch.tensor(c_ids).unsqueeze(0)\n",
        "\n",
        "# Encode the words\n",
        "with torch.no_grad():\n",
        "    a_embedding = model(a_tensor)[0].mean(1)\n",
        "    b_embedding = model(b_tensor)[0].mean(1)\n",
        "    c_embedding = model(c_tensor)[0].mean(1)\n",
        "\n",
        "# Compute the vector difference\n",
        "diff_vector = b_embedding - a_embedding + c_embedding\n",
        "\n",
        "# Find the closest word to the difference vector\n",
        "words = ['লম্বা', 'চারদিকে', 'দূরত্ব']\n",
        "word_ids = tokenizer.convert_tokens_to_ids(words)\n",
        "word_tensor = torch.tensor(word_ids).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    word_embeddings = model(word_tensor)[0].mean(1)\n",
        "similarity = np.dot(word_embeddings, diff_vector.T) / (np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(diff_vector))\n",
        "most_similar_word = words[np.argmax(similarity)]\n",
        "\n",
        "print(f'{a} is to {b} as {c} is to {most_similar_word}')\n"
      ],
      "metadata": {
        "id": "IqknUI6ly65y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "k9ZsWuex0qv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained BanglaT5 tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5\")\n",
        "model = AutoModel.from_pretrained(\"csebuetnlp/banglat5\")\n",
        "\n",
        "# Define the Mikolov-style word analogy\n",
        "a = 'লম্বা'\n",
        "b = 'চারদিকে'\n",
        "c = 'দূরত্ব'\n",
        "\n",
        "# Tokenize the words\n",
        "a_tokens = tokenizer.tokenize(a)\n",
        "b_tokens = tokenizer.tokenize(b)\n",
        "c_tokens = tokenizer.tokenize(c)\n",
        "\n",
        "# Encode the words\n",
        "with torch.no_grad():\n",
        "    a_embedding = model(torch.tensor(tokenizer.encode(a)).unsqueeze(0))[0][0].numpy()\n",
        "    b_embedding = model(torch.tensor(tokenizer.encode(b)).unsqueeze(0))[0][0].numpy()\n",
        "    c_embedding = model(torch.tensor(tokenizer.encode(c)).unsqueeze(0))[0][0].numpy()\n",
        "\n",
        "# Compute the vector difference\n",
        "diff_vector = b_embedding - a_embedding + c_embedding\n",
        "\n",
        "# Find the closest word to the difference vector\n",
        "words = ['লম্বা', 'চারদিকে', 'দূরত্ব']\n",
        "word_embeddings = np.zeros((len(words), len(a_embedding)))\n",
        "for i, word in enumerate(words):\n",
        "    with torch.no_grad():\n",
        "        embedding = model(torch.tensor(tokenizer.encode(word)).unsqueeze(0))[0][0].numpy()\n",
        "        word_embeddings[i] = embedding\n",
        "\n",
        "similarity = np.dot(word_embeddings, diff_vector) / (np.linalg.norm(word_embeddings) * np.linalg.norm(diff_vector))\n",
        "most_similar_word = words[np.argmax(similarity)]\n",
        "\n",
        "print(f'{a} is to {b} as {c} is to {most_similar_word}')\n"
      ],
      "metadata": {
        "id": "Q9wFduTOzXsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install laserembeddings"
      ],
      "metadata": {
        "id": "yhf8dBdVcCSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m laserembeddings download-models"
      ],
      "metadata": {
        "id": "SwH0NdFWcO8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from laserembeddings import Laser\n",
        "\n",
        "laser = Laser()\n",
        "\n",
        "sentence = \"আমি বাংলায় কথা বলি\"\n",
        "embedding = laser.embed_sentences([sentence], lang='bn')[0]\n",
        "\n",
        "embedding = np.array(embedding)\n",
        "print(embedding)"
      ],
      "metadata": {
        "id": "2qaVSFoAb3ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "gDMK5ML2dGZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\")\n",
        "\n",
        "sentences = [\n",
        "    \"আমি বাংলায় কথা বলি\",\n",
        "    \"আমার দেশ বাংলাদেশ\"\n",
        "]\n",
        "\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "    sentence_embeddings = model_output.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "print(sentence_embeddings)\n"
      ],
      "metadata": {
        "id": "5YttxiztcY6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = ['hello', 'world', 'how', 'are', 'you']\n",
        "with open('filename.txt', 'w') as file:\n",
        "    file.write(str(my_list))"
      ],
      "metadata": {
        "id": "k1Hng5XkSqW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Format for Table and Graph in Paper"
      ],
      "metadata": {
        "id": "p9ye15tAZG64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Data\n",
        "models = ['Mikolov-Filtered', 'Our Dataset (Overall)']\n",
        "embeddings = ['Word2Vec', 'GloVe', 'fastText','LABSE', 'bnBERT', 'LASER',  'bnBART']\n",
        "mikolov_data = [0.9, 1.1, 0.3,  4.4, 0.0, 1.6,  1.6]\n",
        "our_data = [0.3, 0.2, 0.6,  6.2, 0.1, 1.1, 11.9]\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Define colors for the bars\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
        "\n",
        "# Define patterns for the bars\n",
        "patterns = cycle(['\\\\', '*','\\\\', '.', '*', '/'])\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.bar([e for e in range(len(embeddings))], mikolov_data, width=0.2, hatch=next(patterns), label='Mikolov-trans. Dataset',color=colors[2])\n",
        "plt.bar([e + 0.2 for e in range(len(embeddings))], our_data, width=0.2, hatch=next(patterns), label='Our Dataset', color=colors[5])\n",
        "\n",
        "# Add labels and titles\n",
        "plt.xlabel('Embedding', fontsize=14)\n",
        "plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "plt.xticks([e + 0.2 for e in range(len(embeddings))], embeddings, fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "# Adjust spacing\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('comparison_top_1.pdf', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fdLZXzjp07Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "models = ['Mikolov-Filtered', 'Our Dataset (Overall)']\n",
        "embeddings = ['Word2Vec', 'GloVe', 'fastText',  'LABSE', 'bnBERT', 'LASER',  'bnBART']\n",
        "mikolov_data = [9.3, 10.0, 0.9,  10.6, 0.0,  6.8, 3.5]\n",
        "our_data = [8.2, 6.3, 3.0,  18.0, 0.4, 10.0,  15.8]\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Define colors for the bars\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
        "\n",
        "# Define patterns for the bars\n",
        "patterns = cycle(['\\\\', '*','\\\\', '.', '*', '/'])\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.bar([e for e in range(len(embeddings))], mikolov_data, width=0.2, hatch=next(patterns), label='Mikolov-trans. Dataset',color=colors[2])\n",
        "plt.bar([e + 0.2 for e in range(len(embeddings))], our_data, width=0.2, hatch=next(patterns), label='Our Dataset', color=colors[5])\n",
        "\n",
        "# Add labels and titles\n",
        "plt.xlabel('Embedding', fontsize=14)\n",
        "plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "plt.xticks([e + 0.2 for e in range(len(embeddings))], embeddings, fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "# Adjust spacing\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('comparison_top_5.pdf', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ksZ6vjnp20nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Data\n",
        "embeddings = ['Word2Vec', 'GloVe', 'fastText',  'LABSE', 'bnBERT', 'LASER', 'bnBART']\n",
        "top1 = [0.5, 0.2, 0.1,  3.8, 0.1, 0.4, 2.8]\n",
        "top5 = [9.0, 7.7, 1.3,  12.8, 0.4, 5.4,  5.4]\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
        "\n",
        "# Define patterns for the bars\n",
        "patterns = cycle(['\\o', '+/','\\\\', '.', '*', '/'])\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.bar([e for e in range(len(embeddings))], top1, width=0.2, hatch=next(patterns), label='Top-1')\n",
        "plt.bar([e + 0.2 for e in range(len(embeddings))], top5, width=0.2, hatch=next(patterns), label='Top-5')\n",
        "\n",
        "# Add labels and titles\n",
        "plt.xlabel('Embedding', fontsize=14)\n",
        "plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "# plt.title('Semantic Accuracy by Embedding', fontsize=14)\n",
        "plt.xticks([e + 0.1 for e in range(len(embeddings))], embeddings, fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.legend(fontsize=14, loc='upper left')\n",
        "\n",
        "# Adjust spacing\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('semantic_top_1_5.pdf', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jhGI-SvCZLKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Data\n",
        "embeddings = ['Word2Vec', 'GloVe', 'fastText',  'LABSE', 'bnBERT', 'LASER',  'bnBART']\n",
        "top1 = [0.1, 0.1, 1.0,8.5, 0.0, 1.7,  21.0]\n",
        "top5 = [7.3, 4.9, 4.6,  23.1, 0.3, 14.5,  26.1]\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Define patterns for the bars\n",
        "patterns = cycle(['\\o', '+/','\\\\', '.', '*', '/'])\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.bar([e for e in range(len(embeddings))], top1, width=0.2, hatch=next(patterns), label='Top-1')\n",
        "plt.bar([e + 0.2 for e in range(len(embeddings))], top5, width=0.2, hatch=next(patterns), label='Top-5')\n",
        "\n",
        "# Add labels and titles\n",
        "plt.xlabel('Embedding', fontsize=14)\n",
        "plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "# plt.title('Syntactic Accuracy by Embedding', fontsize=14)\n",
        "plt.xticks([e + 0.1 for e in range(len(embeddings))], embeddings, fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.legend(fontsize=14, loc='upper left')\n",
        "\n",
        "# Adjust spacing\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('syntactic_top_1_5.pdf', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TNLwmBMTd04h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Data\n",
        "embeddings =['Word2Vec', 'GloVe', 'fastText',  'LaBSE', 'bnBERT', 'LASER','bnBART']\n",
        "top3 = [5.8,4.2,0.5,9.4,0.2,3.4,4.6]\n",
        "top10 = [14.8,13.0,2.7,16.4,0.6,8.2,6.5]\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Define patterns for the bars\n",
        "patterns = cycle(['/.','\\o', '+/','\\\\', '.', '*', '/'])\n",
        "\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#000000']\n",
        "\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.bar([e for e in range(len(embeddings))], top3, width=0.2, hatch=next(patterns), label='Top-3', color = colors[-3])\n",
        "plt.bar([e + 0.2 for e in range(len(embeddings))], top10, width=0.2, hatch=next(patterns), label='Top-10',color = colors[-2])\n",
        "\n",
        "# Add labels and titles\n",
        "plt.xlabel('Embedding', fontsize=14)\n",
        "plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "# plt.title('Semantic Accuracy by Embedding', fontsize=14)\n",
        "plt.xticks([e + 0.1 for e in range(len(embeddings))], embeddings, fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.legend(fontsize=14, loc='upper right')\n",
        "\n",
        "# Adjust spacing\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('semantic_top_3_10.pdf', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QtD9U5xLTBku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Data\n",
        "embeddings =['Word2Vec', 'GloVe', 'fastText',  'LaBSE', 'bnBERT', 'LASER','bnBART']\n",
        "top3 = [4.4,3.3,2.9,19.2,0.2,11.9,24.5]\n",
        "top10 = [9.3,6.2,7.3,28.2,0.4,19.3,27.6]\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Define patterns for the bars\n",
        "patterns = cycle(['/.','\\o', '+/','\\\\', '.', '*', '/'])\n",
        "\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#000000']\n",
        "\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.bar([e for e in range(len(embeddings))], top3, width=0.2, hatch=next(patterns), label='Top-3', color = colors[-3])\n",
        "plt.bar([e + 0.2 for e in range(len(embeddings))], top10, width=0.2, hatch=next(patterns), label='Top-10',color = colors[-2])\n",
        "\n",
        "# Add labels and titles\n",
        "plt.xlabel('Embedding', fontsize=14)\n",
        "plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "# plt.title('Semantic Accuracy by Embedding', fontsize=14)\n",
        "plt.xticks([e + 0.1 for e in range(len(embeddings))], embeddings, fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.legend(fontsize=14, loc='upper left')\n",
        "\n",
        "# Adjust spacing\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('syntactic_top_3_10.pdf', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-u6aCGbHXUZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Data\n",
        "models = ['Mikolov-Filtered', 'Our Dataset (Overall)']\n",
        "embeddings = ['Word2Vec','GloVe','fastText',  'LaBSE', 'bnBERT', 'LASER','bnBART']\n",
        "mikolov_data = [6.1,6.6,0.6,8.6,0.0,4.9,2.8]\n",
        "our_data = [5.1,3.8,1.7,14.3,0.2,7.7,14.6]\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Define colors for the bars\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#000000']\n",
        "\n",
        "\n",
        "# Define patterns for the bars\n",
        "patterns = cycle(['\\\\', '*','\\\\', '.', '*', '/'])\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.bar([e for e in range(len(embeddings))], mikolov_data, width=0.2, hatch=next(patterns), label='Mikolov-trans. Dataset',color=colors[-5])\n",
        "plt.bar([e + 0.2 for e in range(len(embeddings))], our_data, width=0.2, hatch=next(patterns), label='Our Dataset', color=colors[-10])\n",
        "\n",
        "# Add labels and titles\n",
        "plt.xlabel('Embedding', fontsize=14)\n",
        "plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "plt.xticks([e + 0.2 for e in range(len(embeddings))], embeddings, fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "# Adjust spacing\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('comparison_top_3.pdf', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wliBOBUcZss3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Data\n",
        "models = ['Mikolov-Filtered', 'Our Dataset (Overall)']\n",
        "embeddings = ['Word2Vec','GloVe', 'fastText', 'LaBSE', 'bnBERT', 'LASER','bnBART']\n",
        "mikolov_data = [15.2, 15.0, 1.7, 13.1, 0.1, 10.1, 4.4]\n",
        "our_data = [12.1, 9.6, 5.0, 22.3, 0.5, 13.8, 17.1]\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Define colors for the bars\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#000000']\n",
        "\n",
        "\n",
        "# Define patterns for the bars\n",
        "patterns = cycle(['\\\\', '*','\\\\', '.', '*', '/'])\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.bar([e for e in range(len(embeddings))], mikolov_data, width=0.2, hatch=next(patterns), label='Mikolov-trans. Dataset',color=colors[-5])\n",
        "plt.bar([e + 0.2 for e in range(len(embeddings))], our_data, width=0.2, hatch=next(patterns), label='Our Dataset', color=colors[-10])\n",
        "\n",
        "# Add labels and titles\n",
        "plt.xlabel('Embedding', fontsize=14)\n",
        "plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "plt.xticks([e + 0.2 for e in range(len(embeddings))], embeddings, fontsize=13)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "# Adjust spacing\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('comparison_top_10.pdf', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-Zf-SvqLbTFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merge Graph"
      ],
      "metadata": {
        "id": "IjARXf2dkq7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Data\n",
        "embeddings = ['Word2Vec', 'GloVe', 'fastText', 'LABSE', 'bnBERT', 'LASER', 'bnBART']\n",
        "top1 = [0.5, 0.2, 0.1, 3.8, 0.1, 0.4, 2.8]\n",
        "top3 = [5.8, 4.2, 0.5, 9.4, 0.2, 3.4, 4.6]\n",
        "top5 = [9.0, 7.7, 1.3, 12.8, 0.4, 5.4, 5.4]\n",
        "top10 = [14.8, 13.0, 2.7, 16.4, 0.6, 8.2, 6.5]\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Define patterns for the bars\n",
        "patterns = cycle(['\\o', '+/', '\\\\', '*', '*', '/'])\n",
        "\n",
        "# Define colors for the bars\n",
        "# colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
        "colors = ['#92c6df', '#ffa97e', '#97d7a7', '#808080', '#eb9f9f', '#bfa0d4', '#c3a697', '#f0bdd6']\n",
        "\n",
        "# Plot the bar chart for Top-1\n",
        "plt.bar([e for e in range(len(embeddings))], top1, width=0.15, hatch=next(patterns), label='Top-1', color=colors[0])\n",
        "# Plot the bar chart for Top-3\n",
        "plt.bar([e + 0.15 for e in range(len(embeddings))], top3, width=0.15, hatch=next(patterns), label='Top-3', color=colors[1])\n",
        "# Plot the bar chart for Top-5\n",
        "plt.bar([e + 0.3 for e in range(len(embeddings))], top5, width=0.15, hatch=next(patterns), label='Top-5', color=colors[2])\n",
        "# Plot the bar chart for Top-10\n",
        "plt.bar([e + 0.45 for e in range(len(embeddings))], top10, width=0.15, hatch=next(patterns), label='Top-10', color=colors[3])\n",
        "\n",
        "# Add labels and titles\n",
        "plt.xlabel('Embedding', fontsize=18)\n",
        "plt.ylabel('Accuracy (%)', fontsize=18)\n",
        "plt.xticks([e + 0.225 for e in range(len(embeddings))], embeddings, fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.legend(fontsize=16, loc='upper right')\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('semantic_all.pdf', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B9Sg2ssIkqYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Average"
      ],
      "metadata": {
        "id": "er9kReSkwV7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "\n",
        "# Data\n",
        "embeddings = ['Word2Vec', 'GloVe', 'fastText', 'LABSE', 'bnBERT', 'LASER', 'bnBART']\n",
        "top1 = [0.1, 0.1, 1.0, 8.5, 0.0, 1.7, 21.0]\n",
        "top3 = [4.4, 3.3, 2.9, 19.2, 0.2, 11.9, 24.5]\n",
        "top5 = [7.3, 4.9, 4.6, 23.1, 0.3, 14.5, 26.1]\n",
        "top10 = [9.3, 6.2, 7.3, 28.2, 0.4, 19.3, 27.6]\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Define patterns for the bars\n",
        "patterns = cycle(['\\o', '+/', '\\\\', '*', '.', '/'])\n",
        "\n",
        "# Define colors for the bars\n",
        "# colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
        "colors = ['#92c6df', '#ffa97e', '#97d7a7', '#808080', '#eb9f9f', '#bfa0d4', '#c3a697', '#f0bdd6']\n",
        "\n",
        "\n",
        "# Plot the bar chart for Top-1\n",
        "plt.bar([e for e in range(len(embeddings))], top1, width=0.15, hatch=next(patterns), label='Top-1', color=colors[0])\n",
        "# Plot the bar chart for Top-3\n",
        "plt.bar([e + 0.15 for e in range(len(embeddings))], top3, width=0.15, hatch=next(patterns), label='Top-3', color=colors[1])\n",
        "# Plot the bar chart for Top-5\n",
        "plt.bar([e + 0.3 for e in range(len(embeddings))], top5, width=0.15, hatch=next(patterns), label='Top-5', color=colors[2])\n",
        "# Plot the bar chart for Top-10\n",
        "plt.bar([e + 0.45 for e in range(len(embeddings))], top10, width=0.15, hatch=next(patterns), label='Top-10', color=colors[3])\n",
        "\n",
        "# Add labels and titles\n",
        "plt.xlabel('Embedding', fontsize=18)\n",
        "plt.ylabel('Accuracy (%)', fontsize=18)\n",
        "plt.xticks([e + 0.225 for e in range(len(embeddings))], embeddings, fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.legend(fontsize=16, loc='upper left')\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('syntactic_all.pdf', bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0JGmcAMlkprZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jL-ByhQ0kodt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"fasttext\t0.1\t1.3\t0.5\t2.7\n",
        "word2vec\t0.5\t9.0\t5.8\t14.8\n",
        "glove\t0.2\t7.7\t4.2\t13.0\n",
        "laser\t0.4\t5.4\t3.4\t8.2\n",
        "labse\t3.8\t12.8\t9.4\t16.4\n",
        "bnBERT\t0.1\t0.4\t0.2\t0.6\n",
        "bnTrans\t2.8\t5.4\t4.6\t6.5\n",
        "fasttext\t1.0\t4.6\t2.9\t7.3\n",
        "word2vec\t0.1\t7.3\t4.4\t9.3\n",
        "glove\t0.1\t4.9\t3.3\t6.2\n",
        "laser\t1.7\t14.5\t11.9\t19.3\n",
        "labse\t8.5\t23.1\t19.2\t28.2\n",
        "bnBERT\t0.0\t0.3\t0.2\t0.4\n",
        "bnTrans\t21.0\t26.1\t24.5\t27.6\n",
        "\"\"\"\n",
        "\n",
        "lines = text.split('\\n')\n",
        "data = [line.split('\\t') for line in lines]\n",
        "data = [[row[0]] + [float(x) for x in row[1:]] for row in data]\n",
        "print(data)"
      ],
      "metadata": {
        "id": "29iKXSJDYPoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import statistics\n",
        "\n",
        "data = data\n",
        "res = []\n",
        "\n",
        "embed = ['fasttext', 'word2vec', 'glove', 'laser', 'labse', 'bnBERT', 'bnTrans']\n",
        "\n",
        "for e in embed:\n",
        "  embed_data = [row[1:] for row in data if row[0] == e]\n",
        "  avg = [statistics.mean(col) for col in zip(*embed_data)]\n",
        "\n",
        "  res.append(avg)\n",
        "\n",
        "# create a list of rows\n",
        "rows = []\n",
        "for i in range(len(embed)):\n",
        "    row = [embed[i]] + res[i]\n",
        "    rows.append(row)\n",
        "\n",
        "columns = ['Model', 'Top-1', 'Top-3', 'Top-5','Top-10']\n",
        "# create the dataframe\n",
        "df = pd.DataFrame(rows, columns=columns)\n",
        "\n",
        "df.to_excel('example.xlsx', index=False)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "8boEIPOaLch4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write text to excel file"
      ],
      "metadata": {
        "id": "E9e2Y7-kwNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"\n",
        "Affix_বিভক্তি chatgpt 26.32% 26.32% 26.32% 28.95%\n",
        "Affix_বিভক্তি bard 10.53% 18.42% 21.05% 21.05%\n",
        "Tense chatgpt 8.33% 11.81% 13.89% 17.36%\n",
        "Tense bard 1.39% 4.17% 9.03% 11.11%\n",
        "\"\"\"\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "data_list = [line.split() for line in lines]\n",
        "\n"
      ],
      "metadata": {
        "id": "oRtg2RUN2Y2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "\n",
        "# Create a new workbook\n",
        "workbook = openpyxl.Workbook()\n",
        "\n",
        "# Select the active worksheet\n",
        "worksheet = workbook.active\n",
        "\n",
        "# Define the headers\n",
        "headers = ['Task', 'Embedding', 'Top-1 (%)', 'Top-3 (%)', 'Top-5 (%)', 'Top-10 (%)']\n",
        "\n",
        "# Write the headers to the first row\n",
        "worksheet.append(headers)\n",
        "\n",
        "# Write the data to the worksheet\n",
        "data = data_list\n",
        "\n",
        "for row in data:\n",
        "    worksheet.append(row)\n",
        "\n",
        "# Save the workbook\n",
        "workbook.save('data.xlsx')\n"
      ],
      "metadata": {
        "id": "rl03vCmBCK6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bntransformer"
      ],
      "metadata": {
        "id": "n_06zsxHSi02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "dOr4KuItSnNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bntransformer import BanglaTokenizer\n",
        "import transformers\n",
        "\n",
        "model_path = \"facebook/bart-large-cnn\"\n",
        "\n",
        "bntokenizer = BanglaTokenizer(model_path=model_path)\n",
        "\n",
        "text = \"আমি বাংলায় গান গাই ।\"\n",
        "tokens = bntokenizer.tokenize(text)\n",
        "encode_ids = bntokenizer.encode(text)\n",
        "print(encode_ids)\n",
        "\n",
        "# # Get the word embeddings of each token using the pre-trained model\n",
        "# word_embeddings = []\n",
        "# for token_id in encode_ids:\n",
        "#     token = bntokenizer.decode([token_id])\n",
        "#     emb = model.get_word_vector(token)\n",
        "#     word_embeddings.append(emb)\n",
        "\n",
        "# print(word_embeddings)"
      ],
      "metadata": {
        "id": "2qCkTiToSfPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bntransformer import BanglaTokenizer\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# Load the BART model\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "model = transformers.BartModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Load the BanglaTokenizer\n",
        "tokenizer = BanglaTokenizer(model_path=model_name)\n",
        "\n",
        "# Tokenize the input text\n",
        "input_text = \"আমি\"\n",
        "tokens = tokenizer.tokenize(input_text)\n",
        "print(tokens)\n",
        "\n",
        "# Convert the tokens to token IDs\n",
        "input_ids = tokenizer.encode(input_text)\n",
        "\n",
        "# Convert the input_ids to a PyTorch tensor\n",
        "input_tensor = torch.tensor(input_ids)\n",
        "\n",
        "# Generate the word embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_tensor.unsqueeze(0))\n",
        "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "embeddings = torch.mean(embeddings, dim=0)\n",
        "\n",
        "print(embeddings.shape)\n"
      ],
      "metadata": {
        "id": "RsEpqQboUl6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru5TFb-MjrNB"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}